\begin{otherlanguage}{english}
\begin{abstract}

Web accessibility is an issue on many websites, hindering people with disabilities to perceive information on the web. Accessibility evaluation based on existing accessibility guidelines helps to find the barriers that affect these users. A comprehensive accessibility conformance review requires manual labor. However, Accessibility Evaluation Tools (AET) help find the most common accessibility barriers. This thesis explores the potential of Large Language Models (LLM) in evaluating the Web Content Accessibility Guideline (WCAG) success criterion 2.4.2, Page Titled, using pre-made HTML test cases provided by the WCAG Accessibility Conformance Testing rules. Utilizing a Design Science Research method, the input prompt is iterated to evaluate whether the LLM can accurately assess if the HTML code contains a title, whether the title describes the page content, and whether the title identifies the page. The findings reveal that the LLM can perform these evaluations, suggesting that it can be used as an assistant in conformance reviews, potentially speeding up the conformance review process and reducing the need to understand the website's subject matter. However, the study acknowledges the simplicity of the test cases and the non-deterministic nature of LLMs. Future research on LLMs evaluating web accessibility should address language diversity. In addition, implementing LLMs into AETs would provide insight into how LLMs affect the efficiency of the accessibility evaluation process and the capabilities of LLMs on more complex websites.
\end{abstract}
\end{otherlanguage}