\chapter{Web accessibility evaluation\label{accessibility_evaluation}}

As web services undergo rapid development and continuous deployment, a continuous accessibility evaluation is crucial, mandated by both legislative organizations and end-users. The primary goal of assessing web accessibility is to promote digital inclusion by identifying and eliminating barriers, thereby expanding access to a wider audience. Multiple accessibility evaluation tools are available to help web content developers in ensuring inclusivity and conformance to accessibility standards.

\section{Methodologies}

Web accessibility evaluation is a process of assessing and determining to which extent a website is accessible to people with disabilities \citep[Chapter~26.2]{webaccessibility}. The evaluation of web accessibility involves analyzing each web page on the site against established standards and guidelines. The process of web accessibility evaluation can be categorized into three categories: automated testing, manual inspection, and user testing.

Automated testing is done by accessibility evaluation tools that are programmed to automatically parse and evaluate the source code of a web page based on guidelines \citep[Chapter~26.2]{webaccessibility}. Adopting automated testing early ensures that potential accessibility issues are identified and addressed with immediate feedback during the development cycle. However, automated testing tools can only cover those guidelines that are assessable through machine-based analysis.

Manual inspection is required to ensure that a web page conforms to accessibility standards \citep[Chapter~26.2]{webaccessibility}. A conformance review is the most frequently used methodology that involves evaluators checking if the web page meets criteria based on a checklist. \textcite{comparative_accessibility_methods} introduced and compared his Barrier Walkthrough methodology where evaluation is done in a more systematic way taking into consideration the context of the scenario and goal. An example of the scenario and goal is a person with vision impairment, using a specific assistive technology trying to fill in a form. Results show that the Barrier Walkthrough method is better at finding critical problems more accurately, but fails in finding all the accessibility problems \citep{comparative_accessibility_methods}. Nevertheless, a manual inspection conducted by different evaluators may result in different outcomes regardless of the methodology used \citep{accessibility_evaluation_experts, 10.1145/1878803.1878813_testability_expertise}. 

User testing is based on empirical usability testing. User testing is the most reliable accessibility evaluation method involving actual users with disabilities performing tasks on a web page \citep[Chapter~26.2]{webaccessibility}. Reliability is the consistency of the outcome regardless of whom performed the test. However, user testing is slow and costly. In addition, it is complex to set up the testing session and to take into account a diverse range of users \citep{comparative_accessibility_methods}

Multiple evaluation methodologies have been proposed combining the three categories described above \citep[Chapter~26.2.1]{webaccessibility}. It is recommended to always use more than one evaluation tool, as the technical implementation of each tool might differ ending up in different results. Manual inspection and user testing should always be done to ensure conformance. However, there is no consensus on how to combine these methods for a comprehensive accessibility study \citep[Chapter~26.2.1]{webaccessibility}.

\section{Automated tools\label{automated_tools}}

Automated Accessibility Evaluation Tools (AET) should be used to check if a web page conforms to accessibility guidelines. An accessibility evaluation tool parses the HTML and CSS code of a web page and checks that the web page conforms to the accessibility standards. These tools are helpful for quick accessibility evaluation. However, human evaluation is always required to check for full conformance as these tools might produce misleading results \citep{wcagevaluationtools}. Therefore, accessibility evaluation tools should be used as assistive evaluators when evaluating the conformance of a website.

There are dozens of tools to choose from, differing in functionality, coverage, and features \citep{tool_list}. Studies show that the amount of errors found on a page varies largely between tools \citep{comparison_10.1145/3371300.3383346, comparison_10.1145/3607720.3607722, tool_analysis_directive}. Where one tool can find a dozen errors, another tool can find thousands on the same page. \textcite{tool_analysis_directive} reviewed the monitoring reports from member states and did a comparative analysis on 10 free-to-use tools. Results show that almost all tools link found problems to WCAG success criteria. However, tools differed in the amount of criteria they checked and in the final representation of the coverage report. Furthermore, the transparency of tools on which success criteria they cover is not clear to users \citep{tool_analysis_directive}.

A method to compare and measure tool effectiveness was proposed by \textcite{Brajnik2004}. The effectiveness of a tool can be measured with correctness, completeness, and specificity. Correctness measures how tools report non-existing problems (false positives). Completeness measures how tools fail to find problems that exist (false negatives). A tool can be considered highly specific if it can detect a wide range of distinct accessibility issues, offering detailed insights into each problem. \textcite{benchmark_aet} studied the coverage, completeness, and correctness of six popular tools showing that a higher amount of accessibility issues on a page gives a higher completeness score, and in contradiction, on highly accessible pages the completeness score drops. In addition, the study shows that tools with high completeness scores report more easily false positives, reducing the correctness \citep{benchmark_aet}. \textcite{tooltransparency} evaluated four tools on their specificity and transparency. Results show that three of the tools share what success criteria and techniques they cover. However, differences were reported on how tools display the result to the user.

To mitigate the uncertainty and instability of accessibility evaluation tools the W3C has created a task force working on Accessibility Conformance Testing (ACT) rules \citep{act_overview}. The purpose of ACT rules is to standardize the interpretation of WCAG documents by creating technology-specific test cases that accessibility tool developers and accessibility methodology developers can use to test their outcomes. For unified reporting, the W3C has developed a resource description framework named Evaluation and Report Language (EARL). EARL is used to report conformance to the ACT rules. In addition, it can be used to combine results from different tools. However, currently, there is no standard on how to report the output of an automatic evaluation tool, and current tools differ in the reporting style \citep{tool_analysis_directive}.

\section{Tool coverage\label{coverage}}

Automated Accessibility Evaluation Tools (AET's) cover only around 20--30\% of all success criteria in the WCAG \citep{govukaccessibilityresults, dequecoverage, webaimmillions}. In a study conducted by \textcite{govukaccessibilityresults} they created a page with 142 accessibility issues and analyzed this page with 12 different tools (Nu HTML Checker excluded, not an AET). The lowest score was 17 \% by Google Accessibility Developer Tool and the highest scoring tool was SortSite with 40 \%. In addition, the coverage of all tools combined managed to find only 100 out of the 142 accessibility barriers, which supports the need to use multiple tools together when evaluating accessibility \citep{govukaccessibilityresults_blog}. \textcite{comparison_10.1145/3371300.3383346} conducted similar research where they found out that evaluating a page with multiple tools raises the coverage by 10--40 \%. However, \textcite{comparison_10.1145/3371300.3383346} did not study how to combine and remove duplicates from the results between each tool. One large developer and vendor of accessibility testing, Deque Systems, believes that the apprehension on the coverage should be based on real findings rather than which success criteria are covered \citep{dequecoverage}. 

Deque states that up to 57 \% of accessibility issues can be found when considering that there are usually multiple violations for one success criterion on a page \citep{dequecoverage}. From over 2000 in-depth accessibility evaluations they showed that in most cases automation finds more issues than manual review. Data in this study is based on A and AA levels of accessibility when WCAG 2.1 was the recommendation by W3C. The obsolete success criterion 4.1.1 Parsing in WCAG 2.2 was one of the most detected by automation with a proportion of 90.28 \% being found automatically. The six types of issues encountered with a significantly high proportion (percentage in brackets) found by automation are the following: 

\begin{itemize}
  \item 3.1.1 Language of page (91.81 \%, 1 995 issues)
  \item 1.4.3 Contrast (Minimum) (83.11 \%, 73 733 issues)
  \item 2.4.1 Bypass blocks (79 \%, 2 001 issues)
  \item 1.1.1 Non-text context (67.57 \%, 16 014 issues)
  \item 4.1.2 Name, role, value (54.42 \%, 26 276 issues)
  \item 1.3.1 Info and relationship (45.17 \%, 16 432 issues)
\end{itemize}

These six success criteria account for up to 52 \% of all accessibility issues found by automation. By removing the obsolete success criterion 4.1.1 from the data the total amount of found issues by automation is 53 \%. However, this study does not account for the six newly added success criteria in WCAG 2.2 Level AA. Moreover, \textcite{dequeaxe4_5} writes in Deque's blog that the only identified success criterion to be testable in WCAG 2.2 according to their promise, that is not returning false positives, is the success criterion 2.5.8 Target size \citep{dequeaxe4_5}. Deque's target is to ensure that the axe-core engine reports zero false positives to ensure that the results can be trusted by developers and accessibility evaluators \citep{dequecoverage}. Axe-core is an open-source accessibility testing engine for web browsers. It is used by millions of Github projects and it also works as the core for Deque's tools and Google Lighthouse. 

As WCAG version 2.2 is the new standard, according to the coverage report by \textcite{dequecoverage} there would be 17 out of 86 success criteria that automation can discover with certainty. An automated accessibility evaluator can determine that a page has a title or an image has an alternative text from the web page source code. However, these tools can not determine if a page title or alternative text for non-text content is descriptive \citep{wcag_checklist}.

\section{Manual evaluation}

Automated accessibility evaluation tools are the first step in finding accessibility problems on a web page. To increase the coverage, manual inspection is required. Manual evaluation is a costly and time-consuming process as each web page on the whole site has to be evaluated separately. Most of the success criteria in the WCAG can only be determined properly by human evaluation, such as criteria related to context. For example, test automation can detect if the alternative (alt) attribute is set for image tags in the HTML code. However, it can not determine if the alternative text describes the image correctly to users \citep{comparison_10.1145/3371300.3383346}. Additionally, expertise does matter when evaluating a page for conformance \citep{10.1145/1878803.1878813_testability_expertise, comparative_accessibility_methods}. In comparison to a novice, an expert in the field of web accessibility is more capable of finding accessibility barriers on a web page.

As there is no standard methodology for manual evaluation, each evaluator has a toolset that they use to evaluate individual pages manually. Using a semi-automated test tool is a great way to increase coverage of found barriers and to guide the evaluator in particular with wizard-based steps guiding the manual evaluation. Semi-automated test tools are used to evaluate a single page in a specific state for accessibility barriers. These tools run automated accessibility evaluations combined with, for example, user input wizards to cover more possible manually evaluated accessibility barriers. 

Two large accessibility evaluation consultant companies, Deque and Siteimprove, have their own paid semi-automated accessibility evaluation tools. \textcite{dequecoverage_semi} extended the same in-depth coverage study mentioned in Section \ref{coverage} on their semi-automated accessibility testing tool. They discovered that the coverage of found accessibility barriers is increased by 23\% when using their wizard-based semi-automated accessibility evaluation tool \citep{dequecoverage_semi}. The guided testing is incorporated into their test automation to help evaluators with a more systematic conformance evaluation. For example, when trying out Deque's tool free version subscription, the guided testing prompts to check if an alternative text for an image or page title is descriptive in a wizard to the evaluator. The page title wizard was the following question: \blockquote{The page title is 'Understanding WCAG 2.2 | WAI | W3C'. Does it accurately describe the purpose of the page?}. This question leaves the decision up to the evaluator to answer. To answer the question reliably, the evaluator needs to read the content of the page and form an understanding of the entire page.

\section{Use of AI}

With the emergence of multiple Generative AI tools for different contexts, such as music, image, or text generation, the use of AI and machine learning models in web accessibility has sparked discussion and gained focus in the field of accessibility. Companies within the accessibility evaluation industry, such as AudioEye and Deque, have incorporated AI features in their tools \citep{deque_igt, boia_improve_accessibility}. In addition, the current investment in AI has started a cautious discussion within the W3C WCAG Working Group where they are following the progress and possible use-cases of AI in web accessibility \citep{ai_wcag_email}. 

One of the most commonly found issues is a missing descriptive alternative text for images \citep{webaimmillions, dequecoverage}. Therefore, the potential of AI in generating alternative text for images is an interesting topic with multiple viewpoints \citep{ai_wcag_email, boia_alt_text, potential_for_ai}. Mozilla is experimenting with a machine learning model that runs locally on the user's computer to generate missing image alternatives when viewing PDF files within the browser \citep{alt_image_mozilla}. Even though there are tools to generate automatic alternative descriptions to images, the descriptions are not always considered appropriate as they do not take into account the context around the image \citep{accessibility_and_ai, boia_alt_text}. Besides, within the accessibility community, some say that AI should work as an assistant that should not make automatic decisions regarding accessibility \citep{ai_wcag_email, accessibility_and_ai}.

A case study conducted by \textcite{10.1145/3594806.3596542_case_study_gpt} showed that the use of ChatGPT in solving accessibility issues was able to provide correction to code and speed up the process of solving accessibility barriers. However, the study was conducted on two websites and the LLM had problems solving subjective issues, such as the color contrast between the background color and text color. \textcite{Lopez2024Turning} conducted an experiment where they tested if an LLM would be able to evaluate three different WCAG success criteria that require manual evaluation. In total, they had 39 ACT cases and a LLM was able to successfully evaluate 34 of them. However, they had to modify the prompt manually for more complex test cases to specify where to look for the correct information in the HTML tags.

