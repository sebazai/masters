\chapter{Web accessibility evaluation\label{accessibility_evaluation}}

As web services undergo rapid development and continuous deployment, a continuous accessibility evaluation is crucial, mandated by both legislative organizations and end-users. The primary goal of assessing web accessibility is to promote digital inclusion by identifying and eliminating barriers, thereby expanding access to a wider audience. Multiple accessibility evaluation tools are available to help web content developers in ensuring inclusivity and conformance to accessibility standards.

\section{Methodologies}

Web accessibility evaluation is a process of assessing and determining to which extent a website is accessible to people with disabilities \citep[Chapter~26.2]{webaccessibility}. The evaluation of web accessibility involves analyzing a website against established standards and guidelines. The process of web accessibility evaluation can be categorized into three categorize: automated testing, manual inspection and user testing.

Automated testing is done by accessibility evaluation tools that are programmed to automatically parse and evaluate the source code of a web page based on guidelines \citep[Chapter~26.2]{webaccessibility}. 
Adopting automated testing early ensures that potential accessibility issues are identified and addressed with immediate feedback during the development cycle. However, automated testing tools can only cover those guidelines that are assessable through machine-based analysis.

Manual inspection is required to ensure that a web page conforms to accessibility standards \citep[Chapter~26.2]{webaccessibility}. Conformance reviews is the most used methodology that involves evaluators opinion when checking if the website meets criteria based on a checklist, such as WCAG. \cite{comparative_accessibility_methods} introduced and compared his Barrier Walkthrough methodology where evaluation is done in a more systematic way taking into consider the context of the scenario and goal. For example a person with vision impairment, using a specific assistive technology trying to fill in a form. Results show that the Barrier Walkthough method is better at finding critical problems more accurately, but fails in finding all the accessibility problems \citep{comparative_accessibility_methods}. Nevertheless, manual inspection conducted by different evaluators may result in different outcome regardless of methodology used \citep{accessibility_evaluation_experts, 10.1145/1878803.1878813}. 

User testing is based on empirical usability testing. User testing is the most reliable accessibility evaluation method involving actual users with disabilities performing tasks on a web page \citep[Chapter~26.2]{webaccessibility}. Reliability is the consistency of the outcome regardless of whom performed the test. However, user testing is slow and costly. In addition, it is complex to set up the testing session and to take into account a diverse range of users \citep{comparative_accessibility_methods}

Multiple evaluation methodologies have been proposed combining the three categories described above \citep[Chapter~26.2.1]{webaccessibility}. It is recommended to always use more then one tool, as the technical implementation of each tool might differ ending up in different results. Manual inspection and user testing should always be done to ensure conformance. However, there is no consensus on how to combine these methods for a comprehensive accessibility study \citep[Chapter~26.2.1]{webaccessibility}.

\section{Automated tools}

Automated Accessibility Evaluation Tools (AET) should be used to check if a web page conforms to accessibility guidelines. An accessibility evaluation tool parses the HTML and CSS code of a web page and checks that the web page conforms to the accessibility standards. These tools are helpful for quick accessibility evaluation. However, human evaluation is always required to check for full conformance as these tools might produce misleading results \citep{wcagevaluationtools}. Therefore, accessibility evaluation tools should be used as assistive evaluators when examining the conformance of a web page.

There are dozens of tools to choose from, differing in functionality, coverage and features \citep{tool_list}. Studies show that the amount of errors found on a page varies largely \citep{comparison_10.1145/3371300.3383346, comparison_10.1145/3607720.3607722, tool_analysis_directive}. Where one tool can find dozen another can find thousands of errors on the same page. \cite{tool_analysis_directive} reviewed the monitoring reports from member states and did a comparative analysis on 10 free to use tools. Results show that almost all tools link found problems to WCAG success criteria. However, tools differed in the amount of criteria they check and the final representation of the coverage report. Furthermore, the transparency of tools on which success criteria they cover is not clear to users \citep{tool_analysis_directive}.

A method to compare and measure tool effectiveness was proposed by \cite{Brajnik2004}. Effectiveness of a tool can be measured with correctness, completeness and specificity. Correctness measures how tools report non-existing problems (false positives). Completeness measures how tools fail to find problems that exist (false negative). A tool can be considered highly specific if it can detect a wide range of distinct accessibility issues, offering detailed insights into each problem. \cite{benchmark_aet} studied the coverage, completeness and correctness of six popular tools showing that higher amount of accessibility issues on a page gives a higher completeness score and in contradiction on highly accessible pages the completeness score drops. In addition, the study shows that tools with high completeness score reports more easily false positives, reducing the correctness \citep{benchmark_aet}. \cite{tooltransparency} evaluated four tools on their specificity and transparency. Findings show that three of the tools share what success criteria and techniques they cover. However, large difference was pinpointed on how tools report issues and display them to the end user.

To mitigate the uncertainty and instability of accessibility evaluation tools the W3C has created a task force working on Accessibility Conformance Testing (ACT) rules \citep{act_overview}. The purpose of ACT rules is to standardize the interpretation of WCAG documents by creating technology specific test cases that accessibility tool developers can use to test their product. For unified reporting, the W3C has developed a resource description framework named Evaluation and Report Language (EARL). EARL is used to report conformance of the ACT rules. In addition, it can be used to combine results from different tools. However, currently there is no standard on how to report the outcome of an automatic evaluation and current tools differ in the reporting \citep{tool_analysis_directive}.

\section{Coverage}

Automated accessibility evaluation tools cover only around 20 - 30 \% of all success criteria in the WCAG \citep{govukaccessibilityresults, webaimmillions, dequecoverage}. In a study conducted by \cite{govukaccessibilityresults} they created a page with 142 accessibility issues and analyzed this page with 12 different tools (Nu HTML Checker excluded, not exactly an AET). The lowest score was 17 \% by Google Accessibility Developer Tool and highest scoring tool was SortSite with 40 \%. In addition, when combining the outcome from all the tools, only 42 out of the 142 accessibility barriers were not found, which supports the need to use multiple tools together when evaluating accessibility even though it is laborious and time consuming \citep{govukaccessibilityresults_blog}. \cite{comparison_10.1145/3371300.3383346} conducted a similar research where they found out that evaluating a page with multiple tools raises the coverage by 10 - 40 \%. However, \cite{comparison_10.1145/3371300.3383346} did not study how to combine and remove duplicates from the results between each tool. One major developer and vendor of accessibility testing, Deque Systems, believes that the apprehension on the coverage should be based on real findings rather than which success criteria are covered \citep{dequecoverage}. 

Deque states that up to 57 \% of accessibility issues can be found when considering that there are usually multiple violations for one success criterion on a page. From over 2000 in-depth accessibility evaluations they showed that in most cases automation finds more issues then manual review. Data in this study is based on A and AA level of accessibility when WCAG 2.1 was the recommendation from W3C. By removing the success criterion 4.1.1 Parsing, that is obsolete in WCAG 2.2, the total amount of found issues from these studies falls by 34 488 to 260 470. The success criterion 4.1.1 was one of the most detected by automation with a proportion of 90.28 \% being found automatically. The six type of issues encountered with a significantly high proportion found by automation are the following: 

\begin{itemize}
  \item 3.1.1 Language of page (91.81 \%, 1 995 issues)
  \item 1.4.3 Contrast (Minimum) (83.11 \%, 73 733 issues)
  \item 2.4.1 Bypass blocks (79 \%, 2 001 issues)
  \item 1.1.1 Non-text context (67.57 \%, 16 014 issues)
  \item 4.1.2 Name, role, value (54.42 \%, 26 276 issues)
  \item 1.3.1 Info and relationship (45.17 \%, 16 432 issues)
\end{itemize}

These six success criteria accounts for up to 52 \% of all accessibility issues found by automation. By removing the obsolete success criterion the total amount of found issues by tools is 53 \%. However, this study does not account for the six newly added success criteria in WCAG 2.2 Level AA. Moreover, Deque writes that the only identified success criterion to be testable and not returning false positives in WCAG 2.2 is 2.5.8 Target size \citep{dequeaxe4_5}. Deque's target is to ensure that the axe-core engine reports zero false positives to ensure that the results can be trusted \citep{dequecoverage}. Axe-core is an open-source accessibility testing engine for web browsers. It is used by millions of Github projects and it also works as the core for Deques own tools and Google Lighthouse. With WCAG 2.2 being the new standard, according to the coverage report by \cite{dequecoverage} there would be only 17 of out 86 success criteria that automation can discover correctly.

\section{Manual evaluation}

Automated accessibility evaluation tools is the first step on finding accessibility problems on a web page. To increase the coverage, manual inspection is required. Manual evaluation is a costly and time consuming process. 

- Talk about which criteria needs to be manually addressed, also review those that are found by automation, are they relevant, as these are boolean, what about context?

\section{Use of AI}

- Talk about LabelDroid

- Mention accessibility overlay tools; https://commission.europa.eu/resources-partners/europa-web-guide/design-content-and-development/accessibility/testing-early-and-regularly/accessibility-overlays_en, might use AI, has been seen to cause more harm.

- More gray literature, for example from Deque?