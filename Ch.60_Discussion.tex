\chapter{Discussion\label{discussion}}

This chapter contains a discussion of the study, implications, limitations, and potential future research. Section 6.1 is a summary of the research questions. Section 6.2 a discussion on the study result and implications are presented. Section 6.3 presents threats to the validity of the study. Section 6.4 promotes potential future research.

\section{Summary of main findings}

Below is a recap of the research questions and a summary of the answers.

\begin{itemize}
    \item \textbf{RQ1.} What are the limitations of web accessibility evaluation tools in assessing compliance with Web Content Accessibility Guidelines (WCAG)?
\end{itemize}

    A multivocal literature review answered the current limitations of web accessibility evaluation tools. Test automation covers 17 out of the 86 success criteria in the WCAG 2.2. However, test automation tools are not capable of thorough evaluation that would meet the WCAG set standard for some of these 17 success criteria. 

    Sufficiently evaluating conformance requires an accessibility specialist evaluation which is a tedious process, as web pages are more complicated than ever. Semi-automated accessibility evaluation tools help evaluators by guiding them through the most common accessibility barriers found on web pages. However, evaluating for a success criterion requiring interpretation of content can end up in a different outcome depending on the evaluators' point of view.
    
\begin{itemize}
    \item \textbf{RQ2.} How does Generative AI assist in addressing these limitations?
\end{itemize}

    Generative AI can be utilized to address these limitations. Large Language Models can assist evaluators in conformance checks that require interpretation of content. On pages with longer content, the automatic context analysis by LLMs could potentially decrease the overall time for conformance reviews. However, as results show, with zero-shot chain of thought prompting, LLMs are not capable of reliably determining inapplicability.

\section{Study result analysis}

This study shows that even though legislation is moving forward in regards to accessibility, the nature of accessibility and accessibility evaluation is complex, and requires expertise from designers, developers, quality assurance, and accessibility reviewers. An accessibility specialist needs to have a thorough knowledge of the WCAG documents, as no accessibility evaluation tool has 100\% coverage. In addition, web developers, designers, and content creators ought to study the same WCAG documents. 

An accessibility evaluation tool developer has to understand the ACT rules and sufficient techniques used to check for conformance, as well as how browsers work, to develop a reliable and robust tool for accessibility evaluators. Transparency of evaluation tools helps the evaluator understand which success criteria it covers and to what extent. Results show that Large Language Models could be utilized to improve the sufficiency, and possibly help ease the conformance review process, when using semi-automated accessibility evaluation tools. 

\subsection{Prompt iteration}

These results build on existing evidence that LLMs are good Zero-Shot reasoners \citep{kojima2023large}. With rigorous prompt iteration, the accuracy and quality of the outcome improved. By evaluating the outcome of the LLM, patterns can be detected where the LLM fails to provide reasoning for checks it performed, giving possible directions for improvements. An example is the order of the conditional checks the LLM should take into account. Therefore, an imperative approach to how the LLM should operate step by step combined with the zero-shot chain of thought improved the quality of the outcome. However, as the second iteration of the artifact went through multiple. Therefore, uncertainty whether moving the fourth rule to be a manually created sentence, or if replacing the question with the zero-shot chain of thought tested by \textcite{kojima2023large}, improved the accuracy and consistency in the second iteration.

\section{Limitations}

The lack of a thorough evaluation, such as surveys or interviews with potential users of this artifact, is a concern of the validity of this study. The evaluation of usefulness is solely based on the empirical findings of the thesis writer. The artifact is a proof of concept and has not been evaluated by accessibility evaluators for usefulness in a accessibility conformance reviews. Therefore, a proof of suitability, evaluating whether the LLM would assist and speed up conformance reviews, is yet to be evaluated that would support the findings of the study. 

Two concerns regarding the chosen LLM are that this study was solely done using the ChatGPT 3.5 user interface due to it being free to use, and that the LLM is a closed-source tool. Therefore, between the iterations, there is no knowledge if there have been any improvements made by OpenAI to the language model. However, according to OpenAI, the ChatGPT 3.5 model was trained only with data available in early 2022 \citep{openai_35}. Therefore, it can be assumed that the test cases available at \textcite{act_rule_g88} should not be part of the LLM knowledge base. Additionally, the replication of this study is challenging, as no date-fixed version of the model was used. In addition, during September 2024 ChatGPT 3.5 became obsolete for free tier users using the user interface, and a new alternative ChatGPT 4o was announced.

Additionally, the characteristics of LLMs, such as non-deterministic output given the same input, or the limited amount of characters that you can input, are a threat to the external validity of the study. This study was conducted using very short code snippets, therefore no input limits were hit. Even though, in the second iteration all the passing and failing test case sequences were correctly evaluated by the LLM, this does not guarantee that the LLM would always correctly evaluate due to the characteristics of LLM being non-deterministic. However, a better design could reduce the input limitation threat. For example, in this study setting, parsing the HTML code, picking the first title element encountered, and the content within the HTML body or main tag would significantly reduce the number of characters sent to the LLM. In addition, the non-deterministic behavior would cause problems if your accessibility evaluation tool promises zero false positives when evaluating for accessibility.

It is beyond the scope of this study to evaluate how the LLM would work if the website language would be other than English or with possible other language models available either provided by someone or running the models locally.

\section{Future research}

Future studies should take into account language used on the website as accessibility barriers are not only limited to English. In addition, as this study was limited to the short ACT test cases, a case study where the artifact has been implemented into a semi-automatic accessibility evaluation tool with a feedback loop from the evaluator using the AI feature, would provide insight on how the LLM is evaluating larger websites with more content. 

