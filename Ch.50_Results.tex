\chapter{Results\label{results}}

This chapter presents the results and answers the research questions. Section \ref{rq1_result} answers the first research question. In Section \ref{rq2_result} we analyze the outcome of the Generative AI model for each iteration based on our evaluation criteria. In the last section, we analyze if the artifact could be generalized to other possible success criteria and ACT rules provided by W3C.

\section{Limitations of web accessibility evaluation tools\label{rq1_result}}

The literature review done in Chapter \ref{accessibility_evaluation} answers RQ1. In total, there are 86 success criteria in the WCAG 2.2 documents. Out of these 86 success criteria, test automation covers reliably (zero false positives) 17. Using manual evaluation with semi-automated web accessibility evaluation tools increases the coverage, and adds to the sufficiency of these 17 found by automation \citep{dequecoverage_semi}.

Research papers on Automated AETs show that there is a significant distribution of the amount of found accessibility barriers on the same page. Studies conducted point out that even experts evaluating the same page can end up with different results. Additionally, the transparency of results by AETs to accessibility evaluators even further adds to the complexity of interpreting which success criteria have been sufficiently evaluated.

Semi-automated web accessibility evaluation tools help in conformance reviews by guiding the evaluator with wizards to further increase coverage \citep{dequecoverage_semi}. However, the outcome of the final evaluation is subjective to the evaluator \cite{10.1145/1878803.1878813_testability_expertise}. To fully conform with some of the success criteria requires knowledge of the context of the web page. Therefore, expertise matters when evaluating web pages using semi-automated AETs.

\section{Capabilities of LLM's for Page Titled evaluation\label{rq2_result}}

This section describes the outcomes of the artifact and answers RQ2. Some of the success criteria have context-based evaluation metrics, such as 2.4.2 Page Titled success criterion. Overall, the results show that Large Language Models are capable of assisting in a conformance review when evaluating context based success criteria. When given conditions to check for in the HTML code snippet provided, the LLM can identify the correct title and content from the ACT test case HTML code. The last iteration of the artifact shows that the LLM understood the assignment and was able to correctly evaluate the passing and failing ACT test cases.

Each iteration is analyzed in its own subsection. Iteration results are evaluated more deeply based on usefulness, accuracy, and consistency. For each iteration, the \textcite{act_rule_g88} ACT test cases are used. Each test case was executed in sequences of five using the ChatGPT (the LLM) user interface ensuring the ChatGPT 3.5, see Appendix \ref{appendix:iterations}. 

\begin{table}[]
\centering
\caption{Table summarizing the accuracy on how well the LLM evaluated conformance on the test cases for each iteration}
\label{table:result_summary}
\begin{tabular}{|l|cc|cc|cc|cc|cc|cc|cl|}
\hline
Test case  & \multicolumn{2}{c|}{P1}    & \multicolumn{2}{c|}{P2}    & \multicolumn{2}{c|}{P3}    & \multicolumn{2}{c|}{F1}    & \multicolumn{2}{c|}{F2}    & \multicolumn{2}{c|}{F3}    & \multicolumn{2}{c|}{I1}    \\ \hline
Iteration  & \multicolumn{1}{c|}{1} & 2 & \multicolumn{1}{c|}{1} & 2 & \multicolumn{1}{c|}{1} & 2 & \multicolumn{1}{c|}{1} & 2 & \multicolumn{1}{c|}{1} & 2 & \multicolumn{1}{c|}{1} & 2 & \multicolumn{1}{c|}{1} & 2 \\ \hline
Sequence 1 & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{}  & \cmark & \multicolumn{1}{c|}{}  & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} &   \\ \hline
Sequence 2 & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{}  & \cmark \\ \hline
Sequence 3 & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark \\ \hline
Sequence 4 & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{}  & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{}  &   \\ \hline
Sequence 5 & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{}  & \cmark & \multicolumn{1}{c|}{}  & \cmark & \multicolumn{1}{c|}{\cmark} & \cmark & \multicolumn{1}{c|}{}  &   \\ \hline
\end{tabular}
\end{table}

Overall, the LLM is capable of evaluating the provided test cases correctly. In both iterations of the artifact, the passing test cases were the most consistent, accurate, and useful. The LLM had issues evaluating the test cases F1 and F2 in the first iteration. However, the second iteration showed improvements for the failing test cases F1 and F2 in regards to accuracy and consistency, see Table \ref{table:result_summary}. However, in both iterations of the artifact, the inapplicable test case results were hard to evaluate as inapplicability had no example in the input, nor was it described in detail. Additionally, there was no improvement for inapplicability in the second iteration.


\subsection{First iteration}

The LLM was capable of evaluating if the title was descriptive based on the content of the web page. However, problems occurred in failing and inapplicable test cases. Passing test cases had a successful outcome based on our evaluation criteria, whereas failing and inapplicable test cases had a variance in consistency and fallacies in reasoning.

\subsubsection{Passed test cases}

When assessing the output in detail for usefulness, the LLM provides reasoning for each four of the rules in the input. However, there P2 test case's fourth sequence states that the code snippet has only one title element, even though the test case has two. 

In the P3 test case, it can be noted that the LLM is not capable of identifying correctly where the title element is present, and in three out of five sequences, it falsely states that the title element is within the head section of the HTML document, even though it is in the body section. However, this is a minor mistake, as most browsers can fix and set the title correctly.

The accuracy of the accessibility evaluation is described in a short paragraph at the end of the output for all the test cases. Each test case and its sequences have the same outcome from a contextual perspective, therefore the LLM is accurate and consistent when evaluating these passed test cases.

\subsubsection{Failing test cases}

Inconsistency and inaccuracy are notable in the failing test cases F1 and F2 sequences. Additionally, the usefulness is questionable as the reasoning for the outcome has fallacies. However, the F3 test case sequence outcomes did not have similar problems as in test cases F1 and F2.

\subsubsection{F1}

F1-1-2, F1-1-3, and F1-1-4 have the correct outcome where the LLM states partial conformance. Additionally, those sequences that succeed in evaluating conformance correctly also provide suggestions for improvement, see Appendix \ref{F1-1}.

The first sequence (F1-1-1) has problems to understand that the whole web page code is provided. In the outcome, there is no mention of clementines being the correct context of the page. Additionally, the same uncertainty of what the LLM is evaluating can be understood from the last paragraph which seems to state conformance:

\blockquote{Overall, based on the provided HTML code and the given web accessibility rules, the web page appears to conform with the criteria. However, further evaluation within the context of the entire website and user experience might be necessary for a comprehensive assessment.}

In the fifth sequence, the LLM explicitly indicates that the page conforms with the 2.4.2 Page Titled success criterion. In addition, the LLM seems to provide an irrational conclusion for the second rule \textit{Check that the title is relevant to the content of the Web page} with an output of:

\blockquote{The title of the web page is "Apple harvesting season". This title seems relevant to the content of the page, which discusses the harvest timing of clementines, a type of fruit.}

\subsubsection{F2}

The test case with two titles, where the first one is incorrect and the second one is correct causes issues to the conclusions made by the LLM. In regards to accuracy, the LLM states conformance in three out of the five sequences. Therefore, three out of five fail to provide an accurate conformance review as this is a test case that has partial conformance.

A closer look at the three not correctly evaluated test sequences indicates that the existence of two titles on the web page code and the fourth rule pointing out to use the first encountered title causes problems for reasoning, see Figure \ref{first_iteration} for rules. The LLM finds the second title on the web page and uses it to evaluate conformance for the first three rules. This ordering of rules indicates that the scope of the fourth rule could be moved further up to have the conditional checking of rules in a logical order.

Below is an example from F2-1-4 output from the LLM on the fourth rule reasoning, which is similar in all three not correctly evaluated cases (F2-1-1, F2-1-4 and F2-1-5): \blockquote{Since browsers typically recognize only the first title element, and in this case, the relevant title is the second one, it aligns with the rule.}. The LLM evaluated the conformance using the second title even thought the rule explicitly said to do the evaluation based on the first title found in the HTML code.

Due to the possible problems described above, the accuracy for this test case is not on a sufficient level, nor the consistency. Therefore, the reasoning that the LLM uses for evaluating conformance has fallacies. However, the LLM points out that the second title would be more relevant.

\subsubsection{F3}

Throughout all sequences, the answer is consistent and all outcomes accurately state partial conformance and provide suggestions on how to improve the title description. The partial conformance from each outcome is based on the first and fourth rule of the input artifact. In other words, the web page has a title and there is only one title present. However, in the first sequence, the check if the page identified by the title is the only one that stands out with the following outcome: \blockquote{Since the title is present within the <head> section of the HTML document, it can be identified by assistive technologies and is used by browsers to identify the page.}

\subsubsection{Inapplicable test case (I1)}

In only two of the sequences, it was able to indicate that the page should not be evaluated as it does not have a proper HTML structure as the title tag is used to describe the SVG element. On other test sequences, it satisfied the success criterion. However, as this is a zero-shot prompt there are no examples of how to determine applicability or inapplicability. Therefore, the outcome of the sequences that try to evaluate this inapplicable test case is of limited use if the consistency can not be improved.

\subsection{Second iteration}

The improvements done in the second iteration of the artifact had a significant effect on the outcome of failing test cases. With the removal of the fourth rule and using it as a separate instruction in the artifact, the usefulness, accuracy, and consistency increased.

\subsubsection{Passed test cases}

In regards to accuracy, all sequences within the passing test cases were evaluated correctly. Therefore, each test case sequence was consistent in regard to accuracy.

Logical reasoning in each outcome on how it evaluated conformance is provided by the LLM. Therefore, the artifact and the outcome appear to be useful for the evaluator. As in the first iteration of the artifact, the LLM still had issues identifying in the P3 test case that the title element is not within the head section. As earlier stated, in the first iteration results, this is a minor flaw that does not affect the result when the descriptive title is within the body tag.

\subsubsection{Failing test cases}

In comparison to the first iteration of the artifact, significant improvements in all evaluation criteria can be identified for the failing test cases. Accuracy and consistency improvements are distinguished from the outputs, as each sequence outcome is perceived as partial conformance. 

\subsubsection{F1}

Each sequence ends up in partial conformance where the first rule check is passed, that is, the web page has a title. In the second sequence, there is a difference in the partial conformance, as it states that it conforms with the third rule that the web page can be identified by the title \blockquote{Apple harvesting season} even though the content is about clementines. However, all, except the second sequence, have the same logical reasoning where the first rule is met that the page has a title, but the title is not descriptive or can not be used to identify the page. Therefore, not comply with the second and third rules.

\subsubsection{F2}

The input artifact explicitly states the following: \blockquote{The scope of given web accessibility rules are limited to only checking the first title element in a web page.}. However, the outcome of each sequence in the F2 test case states that the second title \blockquote{Clementine harvesting season} would conform to all three given rules. Therefore, the usefulness of this output could be questioned if this information is relevant for the evaluator, as the browser recognizes the first title, which in this test case does not conform with all the rule checks.

Additionally, the existence of two titles provides irrelevant conclusions in all of the sequences in the third rule where the LLM is checking if the page is identifiable based on the title. The LLM should check if the web page can be identified based on the first found title and state in some of the sequences that the web page could be identified using both of the titles. Only in the last sequence it explicitly states \blockquote{However, the first <title> element "First title is incorrect" is not helpful for identifying the content.}

Based on the perceived understanding of each outcome, overall, in terms of accuracy and consistency, each outcome ends up in partial conformance which is described in the last paragraph of each outcome. However, the LLM neglects the information that it should only be evaluating the first found title, therefore the usefulness of the outcome could be improved.

\subsubsection{F3}

A direct quote from one of the sequences summarizes the results very well: \blockquote{Overall, while the page meets the first criterion by having a title, it falls short of the other two criteria as the title isn't directly relevant to the content and may not effectively identify the page.}. Therefore, consistency and accuracy are spot on. Additionally, in each sequence, the reasoning for the outcome is similar and the LLM suggests either explicitly or implicitly a more descriptive title for the test case.

\subsubsection{Inapplicable test case (I1)}

Only the second and third sequence indicates successful evaluation for inapplicability as there is no title element for the whole web page, rather the title element is for the SVG element. For example, the first check that the web page has a title is evaluated in the following way: \blockquote{The web page contains a <title> element within the <svg> tag. However, the <title> element is intended for providing a title for the SVG graphic for accessibility purposes, not for the entire web page}.


\subsection{Generalization}

In Design Science Research Methodologies, the generalization of the artifact is important. Generalization is evaluated on how well the artifact works in other similar contexts \citep{design_science_eval}. This section evaluates the possibility of using the artifact for other conformance-checking techniques provided in the WCAG documents.

In its current form, the possibility of using the artifact for other ACT rules is limited because the input prompt is tailored to test if the web page title is descriptive. The artifact is a proof of concept on how LLM evaluates accessibility using the technique provided by \textcite{g88} and modified accessibility support information from \textcite{act_rule_g88}. 

By incorporating the input prompt in a semi-automated accessibility evaluation tool, placeholders in the input prompt could be utilized, therefore possibly improving the generalization of the artifact. However, the artifact is the code for the LLM, and a small change can have a significant change in the quality of the output, as demonstrated even in this study. Therefore, placeholders, choice of words, and small changes in the input prompt require thorough testing and evaluation.

Furthermore, currently, the artifact is not tied to any specific accessibility evaluation tool. The input artifact could be used by any organization and in other Large Language Models, as the input prompt was the artifact iterated upon.

Overall, in its current form, the artifact is the artifact has potential to be used in other contexts, but this would require further iterations and rigorous evaluation.
