\chapter{Results\label{results}}

This chapter presents the results and answers the research questions. Section \ref{rq1_result} answers the first research question. In Section \ref{rq2_result} we analyze the outcome of the Generative AI model for each iteration based on our evaluation criteria. In the last section we analyze if the artifact could be generalized to other possible success criterion and ACT rules provided by W3C.

\section{Limitations of web accessibility evaluation tools\label{rq1_result}}

The literature review done in Chapter \ref{accessibility_evaluation} answers RQ1. In total there are 86 success criteria in the WCAG 2.2. Out of these 86 success criteria, test automation covers reliably (zero false positives) 17 of the 86 success criteria in the WCAG 2.2. Using manual evaluation with semi-automated web accessibility evaluation tools increases the coverage and adds to the sufficiency of these 17 found by automation.

Research pinpoints that the WCAG 2.2 success criteria are hard to interpret correctly by accessibility evaluator tools developers. Research papers on Automated AET's show that there is a significant distribution on the amount of found accessibility barriers on the same page. Studies conducted points out that even experts evaluating the same page can end up in different result. Additionally, the transparency of results by AET's to accessibility evaluators even further adds to the complexity of interpreting which success criteria has been sufficiently evaluated.

Semi-automated web accessibility evaluation tools helps in conformance reviews by guiding the evaluator with wizards to further increase the coverage. However, the outcome of the final evaluation are subjective to the evaluator. To fully conform with some of the success criterion requires knowledge of the context of the web page. Therefore, expertise matter when evaluating web pages using semi-automated AET's.

\section{Capabilities of LLM's for Page Titled evaluation\label{rq2_result}}

This section describes and evaluates the outcomes of the artifact. Results are evaluated based on usefulness, accuracy and consistency. Each iteration is analysed in an own subsection. For each iteration, the \textcite{act_rule_g88} ACT test cases are used. Each test case were executed in sequences of five using the ChatGPT (the LLM) user interface ensuring the ChatGPT 3.5, see Appendix \ref{appendix:iterations}.

\subsection{First iteration}

The LLM was capable of evaluating if the title was descriptive based on the content of the web page. However, problems occurred in failing and inapplicable test cases. Passing test cases had a better outcome based on our evaluation criteria, whereas failing and inapplicable test cases had variance in consistency and fallacies in reasoning.

\subsubsection{Passed test cases}

When assessing the output in detail for usefulness, the LLM provides reasoning for each four of the rules in the input. However, there P2 test case's fourth sequence states that the code snippet has only one title element, even though the test case has two. 

In the P3 test case, it can be noted that the LLM is not capable of identifying correctly where the title element is present, and in three out of five sequences it falsely states that the title element is within the head section of the HTML document, even though it is in the body section. However, this is a minor mistake, as most browsers are able to fix and set the title correctly.

The accuracy regarding the accessibility evaluation is described in a short paragraph at the end of the output for all the test cases. Each test case and its sequences have the same outcome from a contextual perspective, therefore the LLM is accurate and consistent when evaluating these passed test cases.

\subsubsection{Failing test cases}

Inconsistency and inaccuracy is notable in the failing test cases F1 and F2 sequences. Additionally, the usefulness is questionable as the reasoning for the outcome has fallacies. However, the F3 test case sequence outcomes did not have similar problems as in test case F1 and F2.

\subsubsection{F1}

F1-1-2, F1-1-3 and F1-1-4 have the correct outcome where the LLM states partial conformance. Additionally, those sequences that succeed to evaluate conformance correctly also provide suggestion for improvement, see Appendix \ref{F1-1}.

The first sequence (F1-1-1) has problems to understand that the whole web page code is provided. In the outcome, there is no mention about clementines being the correct context of the page. Additionally, the same uncertainty of what the LLM is evaluating is can be understood from the last paragraph that seems to state conformance:

\blockquote{Overall, based on the provided HTML code and the given web accessibility rules, the web page appears to conform with the criteria. However, further evaluation within the context of the entire website and user experience might be necessary for a comprehensive assessment.}

In the fifth sequence the LLM explicitly indicates that the page conforms with the 2.4.2 Page Titled success criterion. In addition, the LLM seems to provide irrational conclusion for the second rule \textit{Check that the title is relevant to the content of the Web page} with an output of:

\blockquote{The title of the web page is "Apple harvesting season". This title seems relevant to the content of the page, which discusses the harvest timing of clementines, a type of fruit.}


\subsubsection{F2}

The test case with two titles, where the first one is incorrect and second one is correct causes issues to the conclusions made by the LLM. In regards of accuracy, the LLM states conformance in three out of the five sequences. Therefore, three out of five fail to provide accurate conformance review as this is a test case that has partial conformance.

A closer look at the three failing test sequences indicate that the existence of two titles on the web page code and the fourth rule being last causes problems for rational reasoning, see Figure \ref{first_iteration} for rules. The LLM finds the second title in the web page and uses it to evaluate conformance. However, browsers usually pick up the first title. This ordering of rules indicates that the scope of the fourth rule could be moved further up to have the rules in a logical order.

The fourth rule reasoning in all three failing cases is similar, here is a snippet from the F2-1-4 outcome: \blockquote{Since browsers typically recognize only the first title element, and in this case, the relevant title is the second one, it aligns with the rule.}

Due to possible problems described above, the accuracy for this test case is not on a sufficient level, nor the consistency. However, a closer look at the reasoning behind the rules shows that the usefulness is still valid, as the LLM points out that the second title would be more relevant. However, the reasoning that the LLM uses for evaluating conformance has fallacies.

\subsubsection{F3}

Throughout all sequences, the answer is consistent and all outcomes accurately states partial conformance and provides suggestion on how to improve the title description. The partial conformance from each outcome is based on the first and fourth rule of the input artifact. In other words, the web page has a title and there is only one title present. However, in the first sequence, the check if the page be identified by the title is the only one that stands out with the following outcome: \blockquote{Since the title is present within the <head> section of the HTML document, it can be identified by assistive technologies and is used by browsers to identify the page.}

\subsubsection{Inapplicable test case (I1)}

In only two of the sequences it was able to indicate that the page should not be evaluated as it does not have a proper HTML structure as the title tag is used describe the SVG element. On other test sequences it satisfied the success criterion. However, as this is a zero shot prompt there are no examples on how to determine applicability or inapplicability. Therefore, the outcome of the sequences that try to evaluate this inapplicable test case is of limited use if the consistency can not be improved.

\subsection{Second iteration}

REFACTOR THIS FORWARD

The improvements done in the second iteration for the artifact had a significant affect on the outcome in failing test cases. With the removal of the fourth rule and using it as a separate instruction in the artifact, the usefulness, accuracy and consistency increased.

\subsubsection{Passed test cases}

In regards of accuracy, all sequences within the passing test cases was evaluated correctly. Therefore, each test cases sequences were consistent in regards of accuracy.

Logical reasoning in each outcome on how it evaluated conformance is provided by the LLM. Therefore, the artifact and the outcome appears to be useful for the evaluator. As in the first iteration of the artifact, the LLM still had issues to identify in the P3 test case that the title element is not within the head section. As earlier stated, in the first iteration results, that is a minor flaw which does not affect the end result if the descriptive title is within the body tag.

\subsubsection{Failing test cases}

In comparison to first iteration of the artifact, significant improvements in all evaluation criteria can be identified for the failing test cases. Accuracy and consistency improvements are distinguished from the outputs, as each sequence outcome is perceived as partial conformance. 

\subsubsection{F1}

Each sequence ends up in partial conformance where the first rule check is passed, that is, the web page has a title. In the second sequence there is a difference in the partial conformance, as it states that it conforms with the third rule that the web page can be identified by the title \blockquote{Apple harvesting season} even though the content is about clementines. However, all other sequences, except the second, have the same logical reasoning where the first rule is met, but the second and third rules are not.

\subsubsection{F2}

The input artifact explicitly states the following: \blockquote{The scope of given web accessibility rules are limited to only checking the first title element in a web page.}. However, the outcome of each sequence in F2 test case states that the second title \blockquote{Clementine harvesting season} would conform to all three given rules. Therefore, the usefulness of this output could be questioned if this information is relevant for the evaluator, as the browser recognizes the first title, which in this test case does not conform with all the rule checks.

Additionally, the existence of two titles provides irrelevant conclusion in all of the sequences for the third rule, as the LLM should check if the web page can be identified based on the first found title and states in some of the sequences that the web page could be identified using both of the titles. Only in the last sequence it explicitly states \blockquote{However, the first <title> element "First title is incorrect" is not helpful for identifying the content.}

Based on the perceived understanding of each outcome, overall, in terms of accuracy and consistency, each outcome ends up in partial conformance that is described in the last paragraph of the outcome. However, the LLM neglects the information that it should only be evaluating the first found title, therefore the usefulness of the outcome could be improved.

\subsubsection{F3}

\subsubsection{Inapplicable test case (I1)}