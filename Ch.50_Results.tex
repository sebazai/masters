\chapter{Results\label{results}}

This chapter presents the results of this thesis. 

\section{Limitations of web accessibility evaluation tools}

The literature review done in Chapter \ref{accessibility_evaluation} answers RQ1. In total there are 86 success criteria in the WCAG 2.2. Out of these 86 success criteria, test automation covers reliably (zero false positives) 17 the 86 success criteria in the WCAG 2.2. Using manual evaluation with semi-automated web accessibility evaluation tools increases the coverage and adds to the sufficiency of these 17 found by automation.

Research indicates that the WCAG 2.2 success criteria are hard to interpret correctly by accessibility evaluator tools developers. Papers on Automated AET's show that there is a significant distribution on the amount of found accessibility barriers on the same page. Studies conducted points out that even experts evaluating the same page can end up in different result. Additionally, the transparency of results by AET's to accessibility evaluators even further adds to the complexity of interpreting which success criteria has been sufficiently evaluated.

Web accessibility evaluation tools helps in conformance reviews by guiding the evaluator with wizards to further increase the coverage. However, the outcome of these wizards are subjective to the evaluator. Therefore, expertise matter when using semi-automated AET's.

\section{Capabilities of LLM's for Page Titled evaluation}

This section describes and evaluates the outcomes of the artifact. Results are evaluted based on usefulness, accuracy and consistency. Each iteration is analysed in an own subsection.

\subsection{First iteration results}

In the first iteration, all the ACT test cases for the \textcite{act_rule_g88} were ran. Each test case was in sequences of five in the ChatGPT user interface, see Appendix \ref{appendix:iterations}. Overall, the LLM was capable of evaluating if the title was descriptive. However, problems occurred in failing and inapplicable test cases.

\subsubsection{Passing test cases}

Most promising results can be seen in the passing test cases P1, P2 and P3. The output has a coherent structure for each test case. When assessing the output in detail, the LLM provides reasoning for each rule in the input. However, the P2 test case's fourth sequence outcome states that the code snippet has only one title element, even though the test case has two. Each final outcome regarding the accessibility evaluation, is described in a short paragraph at the end of the output. All sequences has the same overall outcome, therefore the LLM is consistent when evaluating these passed test cases.

\subsubsection{Failing test cases}

Inconsistency and inaccuracy is found in the failing test cases F1 and F2. Additionally, there are irrational behavior in same output when it is assessing each rule individually. Furthermore, the length of the answers from the Generative AI are longer then in passed case. In comparison to passing test cases, where the output explicitly states conformance, in failing cases the output tends to provide partial conformance or ambiguity when interpreting the outcome. 

\subsubsection{F1 test case}

In the fifth sequence of F1, see \ref{F1-1}, the LLM explicitly indicates that the page conforms with the 2.4.2 Page Titled success criterion. In addition, the LLM seems to provide irrational conclusion for the second rule \textit{Check that the title is relevant to the content of the Web page} with an output of:

\begin{quote}
    The title of the web page is "Apple harvesting season". This title seems relevant to the content of the page, which discusses the harvest timing of clementines, a type of fruit.
\end{quote}

The first sequence of F1 does not explicitly mention if it conforms or not. A closer look on the answer indicates that the LLM does not understand that the whole context is provided, therefore uncertainty in accuracy is visible. However, the uncertainty is also reflected in the overall description asking for further evaluation. Therefore, the answer is relevant from the evaluators perspective who is using this GenAI assistant when evaluating web pages for conformance.

\subsubsection{F2 test case}

The F2 test case seems to be a major problem for the artifact when considering each of the evaluating criteria. Evaluating each output in detail, an irrational behavior is noticeable. The LLM checks that there is a title that is relevant to the content of the page before it should exclude the first title that is not descriptive in this test case according to the fourth rule. This indicates that the scope of the rule could be moved further up to make the four rules in a logical order. However, the test case input talks about \textit{first title element} and looking at the fifth sequence answer: \textit{The rule specifies that browsers generally recognize only the first title element, so it's assumed that "First title is incorrect" would not be considered by browsers, and "Clementine harvesting season" is the effective title.} would indicate that the test case could also be the problem for the LLM if it explicitly drops out the title based on the content, not order. Same characteristics in answer is found in all other except the third sequence.

Due to possible problems described above, the accuracy for this test case is not a sufficient level. However, a closer look at the reasoning behind the rules shows that the usefulness is still valid, as the LLM points out that the second title would be more relevant.

\subsubsection{F3 test case}

Each answer provides useful suggestion on how to improve the title description and the answers are consistent through each sequence. All other sequences explicitly states partial conformance, scoring well in accuracy, however the chosen words for the first sequence \textit{"Overall, while the web page meets the basic requirement of having a title, it could improve its accessibility by making the title more descriptive and directly relevant to the content..."} is the only one that stands out from these answers. 

\subsubsection{Inapplicable test case (I1)}

The inapplicable test case was hard for the LLM to interpret. It was non-consistent and in two sequences it was able to state that the web page lacks a proper title element for the web page. On other test sequences it satisfied the success criterion. However, as this is a zero shot prompt there are no examples on how to determine applicability for the LLM even though it was able to in two of the sequences.