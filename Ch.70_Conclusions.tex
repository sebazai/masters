\chapter{Conclusions\label{conclusions}}

The goal of this study was to find out how sufficiently current accessibility evaluation tools test the Web Content Accessibility Guideline version 2.2 success criteria and to assess the potential of large language models evaluating a context-based accessibility criterion. It is suggested to use multiple accessibility evaluation tools during conformance reviews to improve the coverage of found accessibility barriers. The findings show that large language models, when given conditions to check for in the prompt, can assist in that the HTML code has a title, the title is relevant to the page content, and the title identifies the page. However, a human evaluator must assess the accuracy of the output, particularly by examining how the large language model reached its conclusions based on the specified conditions. Additionally, an accessibility specialist does not necessarily need to be a subject matter expert regarding website content, as a large language model would help evaluate the context-based criteria. Integrating Generative AI, such as large language models, into accessibility evaluation tools could enhance the accuracy and efficiency of conformance evaluation, enabling accessibility reviewers to carry out more comprehensive accessibility assessments.
