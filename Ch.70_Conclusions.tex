\chapter{Conclusions\label{conclusions}}

The goal of this study was to find out how sufficiently current accessibility evaluation tools test the Web Content Accessibility Guideline version 2.2 success criteria and to assess the potential of large language models in these tools during conformance evaluation. It is suggested to use multiple semi-automated and automated accessibility evaluation tools during conformance evaluation to improve the coverage of found accessibility barriers. The findings show that, when given conditions to check for in the prompt, a large language model can assist in that the HTML code has a title, the title is relevant to the page content, and the title identifies the page. However, a human should evaluate the correctness of the output on how the large language model came to its conclusion based on the conditions. Additionally, an accessibility evaluator does not necessarily need to be a subject matter expert regarding website content, as a large language model could help evaluate the context-based criteria. Integrating Generative AI, such as large language models, into accessibility evaluation tools could enhance the accuracy and efficiency of conformance evaluation, enabling accessibility reviewers to carry out more comprehensive accessibility assessments.

